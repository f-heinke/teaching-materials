---
title: "Notebook 1 - Bestimmung der Proteinmasse mittels SDS-PAGE"
subtitle: "Praktikum: Angewandte Bioinformatik" 
author: M.Sc. Florian Heinke (heinke@hs-mittweida.de)
format:
  html:
    theme: lumen
    code-tools:
      source: false
      toggle: false
      caption: none
    code-block-border-left: "#0069B3"
    code-block-bg: true
    toc: true
    toc-location: left
    toc-depth: 2
    fig-format: svg
df-print: paged
lang: de
---

# Die Problemstellung

Die [SDS-PAGE (Sodium Dodecyl Sulfate Polyacrylamide Gel Electrophoresis)](https://de.wikipedia.org/wiki/SDS-PAGE) ist eine kosteneffiziente und einfache Methode zur Bestimmung von Proteinmassen (bzw. deren Peptidketten im speziellen). In diesem Praktikum versuchen wir eine Massebestimmung eines uns ungekannten Proteins. Die folgende Abbildung zeigt den Lichtbild-Scan des SDS-PAGE-Gels nach dem Trennlauf. Links: Der Scan ohne Annotationen, rechts: der Scan mit Annotationen. 

![](data/scan.png){fig-align="center"}


Rechts hervorgehoben sind die *Lanes* der Marker-Proteine (mit zwei Replikaten), inklusive deren Massen, sowie die *Lane* des rohen Zellextrakts und jeweils eine *Lane* mit aufgereinigtem Protein (IAC: aufgereinigt mittels Ionenaustausch-Chromatographie, IAC+HIC: aufgereinigt mittels IAC und anschlie√üender hydrophober Interaktionschromatographie.) Anhand der Laufstrecken der Markerproteine, deren Massen, und den Laufstecken des zu charakterisierenden Proteins (dessen Gelbanden sind mit Pfeilen hervorgehoben), werden wir die Massen des Proteins bestimmen. Die Laufstrecken wurden mittels ImageJ (einen Vorl√§ufer von [Fiji](https://fiji.sc/)) bestimmt. Hierf√ºr laden wir zuerst die n√∂tigen Daten aus den heruntergeladenen Datenquellen, bestimmen anschlie√üend eine Kalibriergerade anhand der Markerproteine, und nutzen die das resultierende lineare Regressionsmodell zur Massesch√§tzung des unbekannten Proteins. 

# Laden der Daten

Wir laden zuerst die Laufweiten und Massen des Markerproteins aus der soeben heruntergeladenen Tabelle:

```{r}
d_marker <- read.table("data/marker_orig.txt", header = T)

```

Hier ist die kleine Tabelle:

```{r}
print(d_marker)
```

Die Spaltenbezeichner stehen f√ºr die Molekulare Masse (engl.: *molecular weight*) und Laufstrecke (engl.: *migration distance*)

Nun laden wir die Laufweiten des unbekannten Proteins.

```{r}
d_prot <- read.table("data/sample_orig.txt", header = T)
```

```{r}
print( d_prot )
```

# Berechnung eines linearen Regressionsmodells

Aus Grundlagenveranstaltungen sollte Ihnen bekannt sein, dass die Laufgeschwindigkeit eines Molek√ºls durch ein Elektrophoresegel nicht-linear mit seiner Masse korreliert. Genauer: Sei $m$ die Masse eines Molek√ºls, dann ist dessen Geschwindigkeit $v$ negativ exponentiell zur Masse. Wir k√∂nnen den Zusammenhang so schreiben:

$$
v \propto \exp(-m)
$$

Je gr√∂√üer und schwerer ein Protein ist, desto langsamer l√§uft es durch das Gel (systematische St√∂rgr√∂√üen in Gelzusammensetzung und Ladungsverteilung im linearisierten Protein lassen wir au√üen vor). Aus dem Zusammenhang zwischen Laufstrecke, Zeit und Geschwindigkeit ($s = vt$) und der Tatsache, dass alle Molek√ºle in einer SDS-PAGE gleichzeitig aufgetrennt werden ("Startzeit" und "Endzeit" sind identisch), folgt:

$$
s \propto \exp(-m)
$$

Diesen Zusammenhang zeigt das folgende Diagramm recht gut.

```{r}
plot(d_marker$MW, d_marker$MD, type = "b", xlab = "Molekulare Masse (kDa)", ylab = "Laufstrecke (pxl)")
```

Entsprechend muss demnach der Logarithmus der Masse linear zur Laufstrecke sein. Berechnen wir den Logarithmus ...


```{r}
d_marker$log.MW <- log( d_marker$MW )
```

und tragen ihn graphisch auf, sehen wir diesen linearen Zusammenhang.


```{r}
plot(d_marker$log.MW, d_marker$MD, type = "b", xlab = "Molekulare Masse (kDa)", ylab = "Laufstrecke (pxl)")

```

Diese Linearit√§t macht man sich zu Nutze wenn man, so wie in unserem Fall, die Masse parallel gelaufener Proteine anhand derer erfassten Laufstrecken bestimmen m√∂chte. Hierf√ºr "invertieren" wir den obigen Zusammenhang und erhalten, dass die logarithmierte Masse eines Proteins, $\log(m)$, durch die folgende lineare Funktion der Laufstrecke $s$ beschrieben werden kann:  

$$
\log(m) = \beta s + \alpha
$$

Die Parameter $\beta$ und $\alpha$ sind die Koeffizienten der linearen Funkion.

Da wir hier experimentelle und somit verrauschte Daten haben, k√∂nnen wir diesen Zusammenhang statistisch nur in N√§herung bestimmen: 

$$
\log(m) \approx \beta s + \alpha
$$

Dieses statistische Problem k√∂nnen wir mit Hilfe eines linearen, einfachen Regressionsmodells angehen. D.h. wir bestimmen jene *Funktionsparameter* $\beta$ und $\alpha$ die "m√∂glichst gut" (maximalwahrscheinlich) die gegebenen Daten erkl√§ren.

Das geht in `R` ganz einfach mit Hilfe der `lm`-Funktion:

```{r}
regrmodel <- lm(log.MW ~ MD, data = d_marker)
```

Printen wir eine Zusammenfassung unseres Modells, erhalten wir viele interessante Kennzahlen, die wir hier allerdings au√üer Acht lassen:

```{r}
summary( regrmodel )
```


F√ºr uns sind allerdings nur die Koeffizienten entscheidend:

```{r}
print( regrmodel )
```

Die Zahl `MD` mit einem Wert von $-0{,}003$ entspricht dem Anstieg der linearen Regressionsfunktion, $\beta$. $\alpha$ ist der Achsenabschnittskoeffizient von $5{,}07$.

# Massebestimmung

## naiv mittels der Koeffizienten

Bestimmen wir die Massesch√§tzungen (Achtung: Es sind zwei Sch√§tzungen, denn wir haben zwei Banden aus unterschiedlichen Aufreinigungsschritten vorliegen!). Das machen wir hier naiv mittels der ermittelten Koeffizienten, bevor wir im n√§chsten Abschnitt `R`-eigene Funktionen verwenden.

Extrahieren wir die Koeffizienten des Modells. Zuerst den Achsenabschnittskoeffizienten $alpha$.

```{r}
alpha <- regrmodel$coefficients["(Intercept)"]
print(alpha)
```

Und nun den Anstieg $\beta$;

```{r}
beta <- regrmodel$coefficients["MD"]
print(beta)
```

Wir erhalten die Sch√§tzungen, in dem wir die lineare Regressionsfunktion $\beta s + \alpha$ anwenden:

```{r}
log_estim_mass <- beta * d_prot$MD + alpha 
print(log_estim_mass)
```

Nur 3 bis 4 kDa? Kann das sein? Nein, denn wir m√ºssen noch exponentieren -- es handelt sich hier n√§mlich um logarithmierte Massesch√§tzungen.

```{r}
estim_mass <- exp( log_estim_mass )
print(estim_mass)
```

Schon eher! Die Sch√§tzungen sind als bei ca. 43 und 45 kDa.

Tragen wir das diagrammatisch auf, in dem wir log-Masse √ºber die Laufstrecke sowie das Regressionsmodell auftragen. Die soeben berechneten Sch√§tzungen, die wenig √ºberraschend auf der Modellgeraden liegen, f√ºgen wir als rote Punkte ein:

```{r}
plot(d_marker$MD, d_marker$log.MW, xlab = "Laufstrecke (pxl)", ylab = "log(Masse)" )
abline( regrmodel, col = 4)
points( d_prot$MD, log_estim_mass, col = 2, pch = 16)
```


Schauen wir uns das auf der nat√ºrlichen, nicht-logarithmierten Skale an: 

```{r}
plot(d_marker$MD, d_marker$MW, xlab = "Laufstrecke (pxl)", ylab = "Masse (kDa)" )
points( d_prot$MD, estim_mass, col = 2, pch = 16)
```


## mittels `R`-eigenen Funktionen

Verwenden wir jetzt den Funktionsraum von `R`. Wir erhalten die selben, logarithmierten Massesch√§tzungen mit Hilfe der `predict`-Funktion. Als Argumente ben√∂tigt diese das eben berechnete Regressionsmodell und die "neuen" Daten, auf welchen das Modell anzuwenden ist.

```{r}
predict(regrmodel, newdata = d_prot)
```

Diese Funktion kann allerdings noch viel mehr: Sie kann uns Konfidenzintervalle zu diesen Sch√§tzungen angeben. Hier berechnen wir die Intervalle zum statistischen Konfidenzniveau von 95 %:

```{r}
predict(regrmodel, newdata = d_prot,interval = "confidence")
```

Wir erhalten die unteren und obere Schranken der Intervalle. Jedes Intervall liegt dabei **symmetrisch um unsere mittlere Punktsch√§tzung**.

Speichern wir dieses Resultat und tragen es graphisch auf:

```{r}
estim_mass2 <- predict(regrmodel, newdata = d_prot,interval = "confidence")
```

```{r}
plot(d_marker$MD, d_marker$log.MW, xlab = "Laufstrecke (pxl)", ylab = "log(Masse)" )
abline( regrmodel, col = 4)
points( d_prot$MD, estim_mass2[,1], col = 2, pch = 16)

segments(x0 = d_prot$MD, 
         y0 = estim_mass2[,"lwr"], 
         y1 = estim_mass2[,"upr"],
         col = 2)

```

Wir sehen jetzt wunderbar die Unsicherheit dieser Sch√§tzungen.

Mit der `predict`-Funktion k√∂nnten wir noch viel mehr machen. Zum Beispiel k√∂nnen wir ein *Konfidenzband* zu unserem Regressionsmodell bestimmen, in der Art, wie wir es h√§ufig in vielen Publikationen und wissenschaftlichen Abhandlungen finden:

```{r}
md_seq <- seq(50, 700, l = 201)
log_em_seq <- predict(regrmodel, newdata = data.frame(MD = md_seq), interval = "confidence")


plot(d_marker$MD, d_marker$log.MW, xlab = "Laufstrecke (pxl)", ylab = "log(Masse)" )
abline( regrmodel, col = 4)

polygon(x = c(md_seq,
              rev(md_seq)),
        y = c(log_em_seq[,"lwr"],
              rev(log_em_seq[,"upr"])),
        col = adjustcolor(4, alpha.f = 0.25),
        border = F)


points( d_prot$MD, estim_mass2[,1], col = 2, pch = 16)

segments(x0 = d_prot$MD, 
         y0 = estim_mass2[,"lwr"], 
         y1 = estim_mass2[,"upr"],
         col = 2)


```

Die eigentliche Arbeit in diesem Diagramm verrichtet die `polygon`-Funktion, die ein Polygon entlang einer hypothetischen Menge von Laufstrecken (`md_seq`) und den dazu vorhergesagten unteren und oberen Konfidenzintervallgrenzen zieht. Man sieht somit sehr gut, wie die Konfidenz des Modells bei geringen und gro√üen Laufstrecken *abnimmt* -- je breiter das Band, desto breiter das Intervall plausibler Werte. Unsere Sch√§tzungen liegen allerdings gut in der "Mitte des Modells", n√§mlich dort, wo konfidentere Aussagen m√∂glich sind. Das ist kein Zufall; das Experiment wurde entsprechend so geplant.

Plotten wir jetzt das Konfidenzintervallband auf der originalen Masse-Skale:

```{r}
em_seq <- exp( log_em_seq )

plot(d_marker$MD, d_marker$MW, xlab = "Laufstrecke (pxl)", ylab = "log(Masse)" )

polygon(x = c(md_seq,
              rev(md_seq)),
        y = c(em_seq[,"lwr"],
              rev(em_seq[,"upr"])),
        col = adjustcolor(4, alpha.f = 0.25),
        border = F)


points( d_prot$MD, exp(estim_mass2[,1]), col = 2, pch = 16)

segments(x0 = d_prot$MD, 
         y0 = exp(estim_mass2[,"lwr"]), 
         y1 = exp(estim_mass2[,"upr"]),
         col = 2)


```

Auf dieser Skale wird der statistische Nachteil einer linearen Kalibriergeraden auf log-Massen deutlich: Die darauffolgende Exponentiation verzerrt die Konfidenzintervalle und die statistische Power der Bestimmungen sinkt! üòü 

Das sehen wir anhand der folgenden Tabelle: 

```{r}
exp(estim_mass2)
```

Tats√§chlich sind die Konfidenzintervalle jeweils 10 kDa breit. Verunsichert sollten wir uns nun fragen: Kann man das nicht besser sch√§tzen? ü§î


## mittels Bayes'scher Statistik

Sind wir mal ehrlich: Wie k√∂nnen die beiden Konfidenzintervalle derartig gro√ü sein? Wir haben immerhin **zwei** Massesch√§tzungen f√ºr **zwei** Laufstreckenmessungen **eines** Proteins erhalten, die zudem jeweils recht **nah beieinander liegen**! Wie kann die Unsicherheit somit derart gro√ü sein?

Hier mein Versuch einer informellen Antwort: Das Problem ist eigentlich recht einfach formuliert -- das statistische Modell *"wei√ü" nicht*, dass es sich um das selbe Protein handelt. Dadurch verrechnet es nicht die *vereinte* (man sagt auch *gepoolte*) Information, die beide Laufstecken und daraus resultierende Sch√§tzungen liefern. Wie k√∂nnen wir dieses *Pooling* vornehmen und somit bessere Sch√§tzungen erhalten? Das schauen wir uns jetzt an.

Aber Achtung: In diesem Abschnitt wird es fortgeschritten. Wir verwenden nicht nur einen komplexeren statistischen Ansatz sowie ein daraus weniger zug√§ngliches statistisches Modell, sondern auch eine zus√§tzliche Programmiersprache, um dieses Modell an die Daten fitten zu k√∂nnen. Diese Sprache ist [Stan](https://mc-stan.org/). All das ist nicht pr√ºfungsrelevant!

### das Modell

Statistisch gesehen ist das bis jetzt verwendete lineare Regressionsmodell wie folgt formulierbar:

$$
\log(M) \sim \mathrm{Normal}(\ \mu,\ \sigma),\\
\mu = \beta S + \alpha
$$

Wir nehmen damit an, dass die log-Masse eines Proteins im Erwartungswert, $\mu$, dem linearen Zusammenhang $\beta S + \alpha$ folgt. Der Parameter $\sigma$ gibt um $\mu$ normalverteilte Fehlerstreuung im Sinne einer Standardabweichung an.

Unsere bisher erhaltenen Sch√§tzungen sind nichts anderes als die exponentierten, resultierenden Erwartungswerte f√ºr die gemessenen Laufstecken von 368 und 357 Pixel. 

$$
\mu_1 = \beta \cdot 368 + \alpha \\
\mu_2 = \beta \cdot 357 + \alpha
$$

Um diese beiden Erwartungswerte haben wir die ca. $\pm 5$ kDa Konfidenzintervalle erhalten. **Wir √§ndern den Ansatz nun vollst√§ndig**. Als erstes Hinterfragen wir obigen Zusammenhang, denn die statistische Sch√§tzung auf der Log-Skale f√ºhrt zu Verzerrungen und folglich tr√§gt zur Sch√§tzungsunsicherheit bei. Alternativ nutzen wir jetzt den nat√ºrlichen Zusammenhang zwischen mol. Masse und Laufstrecke. Haben wir oben noch die Porportionalit√§t geschrieben als

$$
s \propto \exp(-m)
$$

f√ºhrt uns dies jetzt einem neuen, nat√ºrlicheren Modell mit vier Parametern:

$$
s \sim \mathrm{Normal}(\exp(-\beta m\ +\ \alpha) + \gamma, \sigma),
$$

Wir werden jetzt diese Parameter anhand der Marker-Massen und -Laufstecken sch√§tzen -- wir fitten das Modell. Anschlie√üend wenden wir das gefittete Modell an die Laufstrecken $s_1$ und $s_2$ des zu charakterisierenden Proteins an. Im Kontext Bayes'scher Statistik ist es v√∂llig unproblematisch den nat√ºrlichen Zusammenhang zwischen den zwei Laufstrecken $s_1$ und $s_2$ und der unbekannten Masse $\mu_0$ wie folgt abzubilden:

$$
s_1 \sim \mathrm{Normal}(\exp(-\beta \mu_0\ +\ \alpha) + \gamma, \sigma),\\
s_2 \sim \mathrm{Normal}(\exp(-\beta \mu_0\ +\ \alpha) + \gamma, \sigma)
$$

Sehen Sie, wie die Information zwischen den Messpunkten hier zusammenflie√üt? Wir haben nun einen *Hyperparameter* $\mu_0$ zu sch√§tzen, der dem *Erwartungswert der Erwartungswerte* entspricht ...

![](https://media.tenor.com/w-PCA2wkMQEAAAAM/mind-blown-shocked.gif){fig-align="center" width=300}

Okay, das klingt wild. Aber letztendlich ist es genau das, was wir wollen: Wenn wir die Verteilung der erhaltenen Erwartungswerte (also Masse-Sch√§tzungen) heranziehen, suchen wir jenen Mittelwert der diese Erwartungswerte m√∂glichst plausibel erkl√§ren kann. Das hei√üt, wir suchen ein $\mu_0$ das *statistisch beide Erwartungswerte poolen kann.* 


### die Modelle in Stan implementiert

**Achtung: Ab hier funktionieren die Code-Chunks nur noch auf dem RStudio-Server, da zus√§tzliche Bibliotheken und Einstellungen ben√∂tigt werden. Ab hier ist nichts mehr Pflicht -- ich m√∂chte Ihnen nur zeigen, was technisch so m√∂glich ist.** Sollten Sie den folgenden Abschnitt auf Ihrem Rechner nachbilden wollen, m√ºssen Sie Stan und dessen R-Schnittstelle sowie u. U. weitere Pakete nachinstallieren. Eine Anleitung finden Sie hier: [https://mc-stan.org/cmdstanr/](https://mc-stan.org/cmdstanr/).

Und nun zur√ºck zum Problem und den Stan-Programmen der Modelle. Sie m√ºssen sie nicht programmieren; aber sie in Maschinen-Code [kompilieren](https://de.wikipedia.org/wiki/Compiler) ist n√∂tig:

```{r}
library("cmdstanr")
library("posterior")
```

**Wenn Sie auf dem RStudio-Server arbeiten, m√ºssen Sie jetzt unbedingt die folgende Code-Zeile ausf√ºhren:**

```{r eval=FALSE}
set_cmdstan_path("/opt/cmdstan/current/")
```

Wir kompilieren das erste Modell, dass die Marker-Laufstecken an die Massen fittet:

```{r}
m_marker <- cmdstan_model("marker.data.model.stan")
```

Damit haben wir das Modell erhalten, mit dem wir die Nicht-linearen Regressionsparameter des nat√ºrlichen Modells anhand der Marker-Daten sch√§tzen k√∂nnen.

Als n√§chstes kompilieren wir das Modell, mit welchem wir anschlie√üend das Pooling von $\mu_1$ und $\mu_2$ vornehmen.


```{r}
m_pooling <- cmdstan_model("pooling.model.stan")
```

Und hier sind die Source Codes:

```{r}
print( m_marker )
```

```{r}
print( m_pooling )
```

Gleich fitten wir das erste Modell. Allerdings muss hier gesagt werden, dass das Fitting deutlich stabiler l√§uft, wenn wir die Daten etwas skalieren. Aus didaktischen Gr√ºnden nehmen wir eine einfache Skalierung vor, in dem wir die Messwerte durch angenehme Referenzwerte, hier 700 Pixel f√ºr Laufweiten und 100 kDa f√ºr mol. Massen, teilen. Das tun wir jetzt und speichern die neuen, skalierten Daten in einer Listenstruktur:

```{r}
md <- list(
  MD = d_marker$MD / 700,
  MW = d_marker$MW / 100
  
)
```

Das Skalieren √§ndert nichts am nat√ºrlichen Zusammenhang zwischen den Daten:

```{r}
plot(md$MW, md$MD)
```

Fitten wir jetzt das Modell. In den n√§chsten Sekunden wird Ihr Computer versuchen, Stichprobenwerte der Modellparameter zu finden. Diese Stichprobenwerte werden dabei derartig gew√§hlt, so dass sie die Verteilungen der Modellparameter empirisch ann√§hern. Wir k√∂nnen sie interpretieren als *plausible* Parameterwerte, die die beobachten Marker-Laufstrecken m√∂glichst gut erkl√§ren k√∂nnen, gegeben die molekularen Marker-Massen und das obige Modell des nat√ºrlichen Zusammenhangs zwischen diesen Gr√∂√üen.

Im folgenden Code-Chunk starten wir dieses *Sampling*, wobei wir eine Stichpobe von 5.000 Werten erzeugen lassen:

```{r samplem1}
f <- m_marker$sample( data = md, 
               chains = 1, # one sampling chain ...
               seed = 1, # starting at fixed value for reproducibility, ...
               adapt_delta = 0.999, # generates cautiously ...
               iter_warmup = 5000, # 5000 samples for warming up 
               iter_sampling = 5000, # and the actual 5000 samples after warmup, we' ll use later.
               refresh = 1000 # The processing status is printed after every 1000 samples
               )
```

Nach wenigen Sekunden haben wir 5,000 Werte erhalten. Hier k√∂nnen Sie in die Parameterwerte hineinschauen ...

```{r}
f
```

... aber so richtig sehen wir darin nichts. Es sind nur Zahlen. Stattdessen visualisieren wir die gefitteten Werte:

```{r}
draws <- as_draws_df(f$draws())

mw_seq <- seq(0, 150, l = 151) / 100
plot(md$MW, md$MD)

# add 100 regressions lines drawn from the 5,000 samples
for(i in 1:100) lines( mw_seq, exp(-draws$beta[i] * mw_seq + draws$alpha[i]) + draws$gamma[i], col = adjustcolor(4, 0.2) )


points(md$MW, md$MD, lwd = 2)

```


Bevor wir das zweite, poolende Modell fitten und unsere finale Massesch√§tzung $\mu_0$ erhalten, betrachten wir die G√ºte des gerade gefitteten Modells. Nehmen wir hierf√ºr die skalierten Laufstrecken des unbekannten Proteins und "stecken" sie in das gefittete Modell. Die 700 ist der selbe, schon oben verwendete Referenzwert. Die ben√∂tigten Parameterwerte √ºbergeben wir der folgenden Listenstruktur gleich mit. Packen wir alles in eine Listenstruktur:

```{r}
md2 <- list(
  # scaled distances of the protein
  MD1 = d_prot$MD[1] / 700,
  MD2 = d_prot$MD[2] / 700,
  
  # samples we just obtained:
  N     = length( draws$beta ),
  beta  = draws$beta,
  alpha = draws$alpha,
  gamma = draws$gamma,
  sigma = draws$sigma
)
```

Rechnen wir die entsprechenden Massesch√§tzungen $\mu_1$ und $\mu_2$ aus, in dem wir den nat√ºrlichen Zusammenhang nach der Masse umstellen. Die Multiplikation mit 100 ist n√∂tig, um die Sch√§tzung auf kDa zur√ºck zu skalieren.

```{r}
mu1 <- 100 * (log(md2$MD1 - draws$gamma) - draws$alpha) / -draws$beta  
mu2 <- 100 * (log(md2$MD2 - draws$gamma) - draws$alpha) / -draws$beta
```

Die folgenden Histogramme zeigen, dass unser nat√ºrlicher Ansatz schon deutlich sicherere Sch√§tzungen liefert. Der Gro√üteil der Werte liegen in Intervallen mit Breiten von nur noch 3 bis 4 kDa.  

```{r}
hist(mu1, breaks = 100)
hist(mu2, breaks = 100)
```

In diesen [Quantilen](https://de.wikipedia.org/wiki/Quantil_(Wahrscheinlichkeitstheorie)) liegen 95 % der Werte:

```{r}
PI1 <- quantile( mu1, probs = c(0.025, 0.975) )
PI2 <- quantile( mu2, probs = c(0.025, 0.975) )

PI1
PI2
```

Grafisch sieht das wie folgt aus:

```{r}
plot(md$MW, md$MD)
for(i in 1:100) lines( mw_seq, exp(-draws$beta[i] * mw_seq + draws$alpha[i]) + draws$gamma[i], col = adjustcolor(4, 0.2) )

segments(y0 = md2$MD1, x0 = PI1[1] / 100, x1 = PI1[2] / 100, col = 2, lwd = 3)
segments(y0 = md2$MD2, x0 = PI2[1] / 100, x1 = PI2[2] / 100, col = 2, lwd = 3)

points(md$MW, md$MD, lwd = 2)
```

Und jetzt folgt das zweite Modell, mit welchem wir "√ºber die roten Segmente im Diagramm" poolen.


### Anwendung des Pooling-Modells

Kurzes Recap bis hierhin: Wir haben bis jetzt Stichprobenwerte der Parameter $\beta$, $\gamma$ $\alpha$ (sowie den Fehlerparameter $\sigma$) des "nat√ºrlichen Zusammenhang-Modells" 

$$
s \sim \mathrm{Normal}(\exp(-\beta m\ +\ \alpha) + \gamma, \sigma),
$$

erhalten.

Diese Stichprobenwerte stecken wir jetzt in das zweite Modell, welches √ºber die gemessenen Laufstrecken $s_1$ und $s_2$ poolt und den f√ºr uns spannenden Parameter $\mu_0$ inferiert. Auch hier werden wir wieder "nur" Stichprobenwerte f√ºr $\mu_0$ erhalten.

$$
s_1 \sim \mathrm{Normal}(\exp(-\beta \mu_0\ +\ \alpha) + \gamma, \sigma),\\
s_2 \sim \mathrm{Normal}(\exp(-\beta \mu_0\ +\ \alpha) + \gamma, \sigma)
$$

Wir fitten das Modell. Dabei erzeugen wir 3.000 Stichprobenwerte f√ºr $\mu_0$

```{r}
f2 <- m_pooling$sample(data = md2, chains = 1, refresh = 1000, iter_warmup = 2000, iter_sampling = 3000, adapt_delta = 0.999)
```

Und hier sind die Stichprobenwerte von $\mu_0$. Achtung: Sie m√ºssen den Wert mit dem Referenzwert von 100 gedanklich multiplizieren.

```{r}
f2
```

```{r}
draws2 <- as_draws_df( f2$draws())
```

Hier ist ein Histogramm des gepoolten $\mu_0$

```{r}
hist(100 * draws2$mu0, breaks = 100)
```

Wir erhalten demnach 41 bis 42 kDa als Sch√§tzung. Das Pooling und der Ansatz, den nat√ºrlichen Zusammenhang zwischen Daten statistisch abzubilden, hat uns zu einer deutlich sichereren Sch√§tzung gef√ºhrt. Fun fact: Eine massensprektrometrische Messung ergab eine Masse von 42 kDa.


::: {.callout-note appearance="simple"}
## Literaturempfehlung

F√ºr alle die sich mit der Materie n√§her auseinandersetzen m√∂chten, empfehle ich dieses Buch, das mit vielen interessanten und eing√§ngigen `R`-Beispielen aufwarten kann: [A. Johnson *et al*, Bayes rules! (2021). CRC Press](https://www.bayesrulesbook.com/)

Ebenfalls spannend unter tiefergehend ist [R. McElreath, Statistical rethinking (2020). Taylor & Francis](https://www.taylorfrancis.com/books/mono/10.1201/9780429029608/statistical-rethinking-richard-mcelreath). F√ºr den Blick unter die (mathematische) Motorhaube, empfehle ich [A. Gelman *et al*, Bayesian Data Analysis (2013, online edition 2025). Chapman & Hall](https://sites.stat.columbia.edu/gelman/book/). 

:::


